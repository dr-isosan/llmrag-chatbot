import os
import json
import re
import hashlib
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import logging

# Document processing libraries
import fitz  # PyMuPDF
from docx import Document

# textract not available - using alternative approach
from config import config

logger = logging.getLogger(__name__)


@dataclass
class DocumentMetadata:
    """DokÃ¼man metadata'sÄ± iÃ§in dataclass"""

    filename: str
    file_type: str
    file_size: int
    creation_date: Optional[str] = None
    modification_date: Optional[str] = None
    author: Optional[str] = None
    title: Optional[str] = None
    page_count: Optional[int] = None
    language: Optional[str] = None
    checksum: Optional[str] = None


@dataclass
class ProcessedChunk:
    """Ä°ÅŸlenmiÅŸ chunk iÃ§in dataclass"""

    content: str
    chunk_index: int
    start_pos: int
    end_pos: int
    chunk_type: str = "content"  # content, header, table, etc.
    metadata: Optional[Dict[str, Any]] = None


class AdvancedDocumentProcessor:
    """GeliÅŸmiÅŸ dokÃ¼man iÅŸleme sÄ±nÄ±fÄ±"""

    SUPPORTED_FORMATS = {
        ".pdf": "PDF",
        ".docx": "DOCX",
        ".doc": "DOC",
        ".txt": "TXT",
        ".rtf": "RTF",
        ".html": "HTML",
        ".htm": "HTML",
        ".md": "MARKDOWN",
    }

    def __init__(self):
        self.chunk_size = config.MAX_CHUNK_SIZE
        self.overlap = config.CHUNK_OVERLAP
        self.stats = {
            "processed_files": 0,
            "failed_files": 0,
            "total_chunks": 0,
            "total_characters": 0,
        }

    def extract_text_from_pdf(self, file_path: str) -> Tuple[str, DocumentMetadata]:
        """GeliÅŸmiÅŸ PDF metin Ã§Ä±karma"""
        try:
            doc = fitz.open(file_path)
            text_parts = []
            metadata = self._extract_pdf_metadata(doc, file_path)

            for page_num in range(len(doc)):
                page = doc[page_num]
                # Metin Ã§Ä±karma
                text = page.get_text()  # type: ignore

                # Tablo tespiti ve Ã§Ä±karma
                tables = page.find_tables() if hasattr(page, "find_tables") else None  # type: ignore
                if tables:
                    for table in tables:
                        try:
                            table_data = (
                                table.extract() if hasattr(table, "extract") else None
                            )
                            table_text = (
                                self._format_table_text(table_data)
                                if table_data
                                else ""
                            )
                            text += f"\n[TABLO {page_num}]\n{table_text}\n[/TABLO]\n"
                        except Exception as e:
                            logger.warning(
                                f"Tablo Ã§Ä±karma hatasÄ± sayfa {page_num}: {e}"
                            )

                # GÃ¶rsel-metin iliÅŸkisi (OCR iÃ§in placeholder)
                images = page.get_images() if hasattr(page, "get_images") else None  # type: ignore
                if images:
                    text += f"\n[GÃ–RSEL SAYISI: {len(images)}]\n"

                if text.strip():
                    text_parts.append(
                        f"[SAYFA {page_num}]\n{text}\n[/SAYFA {page_num}]"
                    )
                else:
                    logger.debug(f"Sayfa {page_num} boÅŸ")

            doc.close()
            full_text = "\n".join(text_parts)
            metadata.page_count = len(text_parts)

            logger.info(
                f"   ğŸ“„ PDF'den {len(text_parts)} sayfa metin Ã§Ä±karÄ±ldÄ±, toplam {len(full_text)} karakter"
            )

            return full_text, metadata

        except Exception as e:
            logger.error(f"PDF okuma hatasÄ± {file_path}: {e}")
            return "", self._create_error_metadata(file_path, str(e))

    def extract_text_from_docx(self, file_path: str) -> Tuple[str, DocumentMetadata]:
        """GeliÅŸmiÅŸ DOCX metin Ã§Ä±karma"""
        try:
            doc = Document(file_path)
            text_parts = []
            metadata = self._extract_docx_metadata(doc, file_path)

            # Paragraflar
            for para in doc.paragraphs:
                if para.text.strip():
                    # BaÅŸlÄ±k tespiti
                    if (
                        para.style
                        and getattr(para.style, "name", None)
                        and str(para.style.name).startswith("Heading")
                    ):
                        heading_name = str(para.style.name)
                        heading_level = (
                            heading_name[-1]
                            if heading_name and heading_name[-1].isdigit()
                            else ""
                        )
                        text_parts.append(
                            f"[BAÅLIK {heading_level}] {para.text} [/BAÅLIK]"
                        )
                    else:
                        text_parts.append(para.text)

            # Tablolar
            for table_num, table in enumerate(doc.tables, 1):
                table_data = []
                for row in table.rows:
                    row_data = [cell.text.strip() for cell in row.cells]
                    table_data.append(row_data)

                table_text = self._format_table_text(table_data)
                text_parts.append(f"\n[TABLO {table_num}]\n{table_text}\n[/TABLO]\n")

            return "\n".join(text_parts), metadata

        except Exception as e:
            logger.error(f"DOCX okuma hatasÄ± {file_path}: {e}")
            return "", self._create_error_metadata(file_path, str(e))

    def extract_text_universal(self, file_path: str) -> Tuple[str, DocumentMetadata]:
        """DiÄŸer formatlar iÃ§in universal Ã§Ä±karma (textract)"""
        try:
            try:
                import textract  # type: ignore
            except ImportError:
                logger.error(
                    "textract kÃ¼tÃ¼phanesi yÃ¼klÃ¼ deÄŸil. Universal extraction kullanÄ±lamaz."
                )
                return "", self._create_error_metadata(
                    file_path, "textract not installed"
                )
            text = textract.process(file_path).decode("utf-8")
            metadata = self._create_basic_metadata(file_path)
            return text, metadata
        except Exception as e:
            logger.error(f"Universal text extraction hatasÄ± {file_path}: {e}")
            return "", self._create_error_metadata(file_path, str(e))

    def _extract_pdf_metadata(
        self, doc: fitz.Document, file_path: str
    ) -> DocumentMetadata:
        """PDF metadata Ã§Ä±karma"""
        try:
            pdf_metadata = getattr(doc, "metadata", None)
            file_stats = os.stat(file_path)

            return DocumentMetadata(
                filename=os.path.basename(file_path),
                file_type="PDF",
                file_size=file_stats.st_size,
                creation_date=(
                    pdf_metadata.get("creationDate", "") if pdf_metadata else None
                ),
                modification_date=(
                    pdf_metadata.get("modDate", "") if pdf_metadata else None
                ),
                author=pdf_metadata.get("author", "") if pdf_metadata else None,
                title=pdf_metadata.get("title", "") if pdf_metadata else None,
                page_count=getattr(doc, "page_count", None),
                checksum=self._calculate_checksum(file_path),
            )
        except Exception as e:
            logger.warning(f"PDF metadata Ã§Ä±karma hatasÄ±: {e}")
            return self._create_basic_metadata(file_path)

    def _extract_docx_metadata(self, doc, file_path: str) -> DocumentMetadata:
        """DOCX metadata Ã§Ä±karma"""
        try:
            core_props = doc.core_properties
            file_stats = os.stat(file_path)

            return DocumentMetadata(
                filename=os.path.basename(file_path),
                file_type="DOCX",
                file_size=file_stats.st_size,
                creation_date=str(core_props.created) if core_props.created else None,
                modification_date=(
                    str(core_props.modified) if core_props.modified else None
                ),
                author=core_props.author,
                title=core_props.title,
                checksum=self._calculate_checksum(file_path),
            )
        except Exception as e:
            logger.warning(f"DOCX metadata Ã§Ä±karma hatasÄ±: {e}")
            return self._create_basic_metadata(file_path)

    def _create_basic_metadata(self, file_path: str) -> DocumentMetadata:
        """Temel metadata oluÅŸturma"""
        file_stats = os.stat(file_path)
        file_ext = Path(file_path).suffix.lower()

        return DocumentMetadata(
            filename=os.path.basename(file_path),
            file_type=self.SUPPORTED_FORMATS.get(file_ext, "UNKNOWN"),
            file_size=file_stats.st_size,
            checksum=self._calculate_checksum(file_path),
        )

    def _create_error_metadata(self, file_path: str, error: str) -> DocumentMetadata:
        """Hata durumu iÃ§in metadata"""
        metadata = self._create_basic_metadata(file_path)
        # 'metadata' alanÄ± yok, ek bilgi iÃ§in checksum'a hata ekle
        metadata.checksum = f"error:{error}"
        return metadata

    def _calculate_checksum(self, file_path: str) -> str:
        """Dosya checksum hesaplama"""
        try:
            hasher = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except Exception as e:
            logger.warning(f"Checksum hesaplama hatasÄ±: {e}")
            return ""

    def _format_table_text(self, table_data: List[List[str]]) -> str:
        """Tablo verilerini metin formatÄ±na Ã§evirme"""
        if not table_data:
            return ""

        formatted_rows = []
        for row in table_data:
            # None hÃ¼creleri boÅŸ stringe Ã§evir
            clean_row = [
                str(cell).strip() if cell is not None else ""
                for cell in row
                if cell is not None and str(cell).strip()
            ]
            if clean_row:
                formatted_rows.append(" | ".join(clean_row))

        return "\n".join(formatted_rows)

    def advanced_clean_text(self, text: str, preserve_structure: bool = True) -> str:
        """GeliÅŸmiÅŸ metin temizleme"""
        if not text:
            return ""

        original_text = text

        # TÃ¼rkÃ§e-spesifik dÃ¼zeltmeler
        turkish_corrections = {
            "Ä°": "i",
            "I": "Ä±",
            "Ä": "ÄŸ",
            "Ãœ": "Ã¼",
            "Å": "ÅŸ",
            "Ã–": "Ã¶",
            "Ã‡": "Ã§",
        }

        # YapÄ±sal etiketleri geÃ§ici olarak koru
        structure_markers = re.findall(r"\[(?:SAYFA|BAÅLIK|TABLO|GÃ–RSEL)[^\]]*\]", text)

        if preserve_structure:
            # YapÄ±sal bilgileri koru
            pass
        else:
            # YapÄ±sal etiketleri kaldÄ±r
            text = re.sub(r"\[(?:SAYFA|BAÅLIK|TABLO|GÃ–RSEL)[^\]]*\]", "", text)

        # Temel temizlik
        # Ã‡oklu boÅŸluklarÄ± tek boÅŸluk yap
        text = re.sub(r"\s+", " ", text)

        # Ã–zel karakterleri temizle (TÃ¼rkÃ§e karakterleri koru)
        text = re.sub(r'[^\w\sÃ§Ã‡ÄŸÄÄ±Ä°Ã¶Ã–ÅŸÅÃ¼Ãœ.,;:!?\-\(\)\[\]"\'\/]', "", text)

        # Gereksiz sayfa numaralarÄ±nÄ± temizle (ama Ã¶nemli sayÄ±larÄ± koru)
        text = re.sub(r"\bSayfa\s*\d+\b", "", text, flags=re.IGNORECASE)
        text = re.sub(r"\bPage\s*\d+\b", "", text, flags=re.IGNORECASE)

        # BaÅŸlÄ±k numaralarÄ±nÄ± kontrollÃ¼ temizle
        # Sadece satÄ±r baÅŸÄ±ndaki numaralandÄ±rmayÄ± temizle
        text = re.sub(r"^\d+(\.\d+)*\s+", "", text, flags=re.MULTILINE)

        # Ã‡oklu satÄ±r sonlarÄ±nÄ± normalize et
        text = re.sub(r"\n\s*\n+", "\n\n", text)

        # BaÅŸÄ±ndaki ve sonundaki boÅŸluklarÄ± temizle
        text = text.strip()

        # SonuÃ§ kontrolÃ¼
        if len(text) < len(original_text) * 0.1:  # %90'dan fazla kayÄ±p varsa
            logger.warning("AÅŸÄ±rÄ± temizlik algÄ±landÄ±, orijinal metin korunuyor")
            return original_text.strip()

        return text

    def adaptive_chunk_creation(
        self, text: str, metadata: DocumentMetadata
    ) -> List[ProcessedChunk]:
        """Adaptif chunk oluÅŸturma"""
        if not text:
            return []

        chunks = []

        # DokÃ¼man tipine gÃ¶re stratejik chunking
        if metadata.file_type == "PDF" and metadata.page_count:
            # Sayfa bazlÄ± chunking
            chunks.extend(self._page_based_chunking(text))
        else:
            # Paragraf bazlÄ± chunking
            chunks.extend(self._paragraph_based_chunking(text))

        # Chunk kalite kontrolÃ¼
        chunks = self._validate_chunks(chunks)

        return chunks

    def _page_based_chunking(self, text: str) -> List[ProcessedChunk]:
        """Sayfa bazlÄ± chunking"""
        chunks = []
        page_pattern = r"\[SAYFA (\d+)\](.*?)\[/SAYFA \1\]"

        page_matches = re.finditer(page_pattern, text, re.DOTALL)
        page_matches_list = list(page_matches)

        logger.info(
            f"   ğŸ” Sayfa bazlÄ± chunking: {len(page_matches_list)} sayfa bulundu"
        )

        if not page_matches_list:
            logger.warning(
                f"   âš ï¸ Sayfa etiketleri bulunamadÄ±, paragraf chunking'e geÃ§iliyor"
            )
            return self._paragraph_based_chunking(text)

        for match in page_matches_list:
            page_num = match.group(1)
            page_content = match.group(2).strip()

            if len(page_content) <= self.chunk_size:
                # Sayfa kÃ¼Ã§Ã¼kse tek chunk
                chunks.append(
                    ProcessedChunk(
                        content=page_content,
                        chunk_index=len(chunks),
                        start_pos=match.start(),
                        end_pos=match.end(),
                        chunk_type="page",
                        metadata={"page_number": page_num},
                    )
                )
            else:
                # BÃ¼yÃ¼k sayfalarÄ± bÃ¶l
                sub_chunks = self._split_large_content(page_content)
                for i, sub_chunk in enumerate(sub_chunks):
                    chunks.append(
                        ProcessedChunk(
                            content=sub_chunk,
                            chunk_index=len(chunks),
                            start_pos=match.start(),
                            end_pos=match.end(),
                            chunk_type="page_part",
                            metadata={"page_number": page_num, "part": i + 1},
                        )
                    )

        return chunks

    def _paragraph_based_chunking(self, text: str) -> List[ProcessedChunk]:
        """Paragraf bazlÄ± chunking"""
        chunks = []
        paragraphs = text.split("\n\n")

        current_chunk = ""
        current_start = 0

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            # EÄŸer mevcut chunk + yeni paragraf Ã§ok bÃ¼yÃ¼kse
            if len(current_chunk) + len(para) > self.chunk_size and current_chunk:
                # Mevcut chunk'Ä± kaydet
                chunks.append(
                    ProcessedChunk(
                        content=current_chunk.strip(),
                        chunk_index=len(chunks),
                        start_pos=current_start,
                        end_pos=current_start + len(current_chunk),
                        chunk_type="paragraph_group",
                    )
                )

                # Yeni chunk baÅŸlat (overlap ile)
                current_chunk = current_chunk[-self.overlap :] + " " + para
                current_start = current_start + len(current_chunk) - self.overlap
            else:
                # Paragrafi ekle
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para

        # Son chunk'Ä± ekle
        if current_chunk.strip():
            chunks.append(
                ProcessedChunk(
                    content=current_chunk.strip(),
                    chunk_index=len(chunks),
                    start_pos=current_start,
                    end_pos=current_start + len(current_chunk),
                    chunk_type="paragraph_group",
                )
            )

        return chunks

    def _split_large_content(self, content: str) -> List[str]:
        """BÃ¼yÃ¼k iÃ§eriÄŸi bÃ¶l"""
        if len(content) <= self.chunk_size:
            return [content]

        chunks = []
        words = content.split()
        current_chunk_words = []
        current_length = 0

        for word in words:
            word_length = len(word) + 1  # +1 for space

            if current_length + word_length > self.chunk_size and current_chunk_words:
                # Chunk'Ä± kaydet
                chunks.append(" ".join(current_chunk_words))

                # Overlap iÃ§in son birkaÃ§ kelimeyi al
                overlap_words = current_chunk_words[
                    -self.overlap // 10 :
                ]  # Approximate overlap
                current_chunk_words = overlap_words + [word]
                current_length = sum(len(w) + 1 for w in current_chunk_words)
            else:
                current_chunk_words.append(word)
                current_length += word_length

        # Son chunk'Ä± ekle
        if current_chunk_words:
            chunks.append(" ".join(current_chunk_words))

        return chunks

    def _validate_chunks(self, chunks: List[ProcessedChunk]) -> List[ProcessedChunk]:
        """Chunk kalite kontrolÃ¼"""
        validated_chunks = []

        for chunk in chunks:
            # Ã‡ok kÄ±sa chunk'larÄ± filtrele
            if len(chunk.content.strip()) < 20:
                logger.debug(f"Ã‡ok kÄ±sa chunk atlandÄ±: {len(chunk.content)} karakter")
                continue

            # Sadece noktalama iÅŸareti iÃ§eren chunk'larÄ± filtrele
            if re.match(r"^[^\w]*$", chunk.content.strip()):
                logger.debug("Sadece noktalama iÅŸareti iÃ§eren chunk atlandÄ±")
                continue

            validated_chunks.append(chunk)

        return validated_chunks

    def process_documents(self, path: str, keyword: str = None) -> List[Dict[str, Any]]:
        """Ana dokÃ¼man iÅŸleme fonksiyonu (anahtar kelime eÅŸleÅŸmesi zorunlu)"""
        if not os.path.exists(path):
            raise FileNotFoundError(f"Dosya veya klasÃ¶r bulunamadÄ±: {path}")

        # Dosya listesi oluÅŸtur
        files_to_process = self._get_supported_files(path)

        if not files_to_process:
            logger.warning(f"Ä°ÅŸlenebilir dosya bulunamadÄ±: {path}")
            return []

        logger.info(f"ğŸ“„ {len(files_to_process)} dosya bulundu. Ä°ÅŸleniyor...")

        processed_data = []

        for i, file_path in enumerate(files_to_process, 1):
            logger.info(
                f"[{i}/{len(files_to_process)}] Ä°ÅŸleniyor: {os.path.basename(file_path)}"
            )

            try:
                # Dosya tipine gÃ¶re iÅŸle
                file_ext = Path(file_path).suffix.lower()

                if file_ext == ".pdf":
                    raw_text, metadata = self.extract_text_from_pdf(file_path)
                elif file_ext in [".docx", ".doc"]:
                    raw_text, metadata = self.extract_text_from_docx(file_path)
                else:
                    raw_text, metadata = self.extract_text_universal(file_path)

                if not raw_text.strip():
                    logger.warning(f"   âš ï¸ BoÅŸ iÃ§erik: {metadata.filename}")
                    self.stats["failed_files"] += 1
                    continue

                # Metni temizle
                cleaned_text = self.advanced_clean_text(raw_text)

                logger.info(
                    f"   ğŸ§¹ Temizleme: {len(raw_text)} -> {len(cleaned_text)} karakter"
                )

                if not cleaned_text.strip():
                    logger.warning(
                        f"   âš ï¸ Temizleme sonrasÄ± boÅŸ iÃ§erik: {metadata.filename}"
                    )
                    self.stats["failed_files"] += 1
                    continue

                # Adaptif chunking
                processed_chunks = self.adaptive_chunk_creation(cleaned_text, metadata)

                logger.info(
                    f"   ğŸ“¦ Chunking sonucu: {len(processed_chunks)} chunk oluÅŸturuldu"
                )

                if not processed_chunks:
                    logger.warning(f"   âš ï¸ Chunk oluÅŸturulamadÄ±: {metadata.filename}")
                    self.stats["failed_files"] += 1
                    continue

                # SonuÃ§ verilerini hazÄ±rla
                chunk_texts = [chunk.content for chunk in processed_chunks]
                chunk_metadata = [chunk.metadata or {} for chunk in processed_chunks]

                result_data = {
                    "filename": metadata.filename,
                    "file_type": metadata.file_type,
                    "file_size": metadata.file_size,
                    "content": cleaned_text,
                    "chunks": chunk_texts,
                    "chunk_metadata": chunk_metadata,
                    "chunk_count": len(chunk_texts),
                    "character_count": len(cleaned_text),
                    "document_metadata": {
                        "creation_date": metadata.creation_date,
                        "modification_date": metadata.modification_date,
                        "author": metadata.author,
                        "title": metadata.title,
                        "page_count": metadata.page_count,
                        "checksum": metadata.checksum,
                    },
                }
                # Anahtar kelimeyi sadece bu dosya iÃ§in ekle
                if keyword is not None and os.path.basename(file_path) == result_data["filename"]:
                    result_data["keyword"] = keyword

                processed_data.append(result_data)

                # Ä°statistik gÃ¼ncelle
                self.stats["processed_files"] += 1
                self.stats["total_chunks"] += len(chunk_texts)
                self.stats["total_characters"] += len(cleaned_text)

                logger.info(
                    f"   âœ… {len(chunk_texts)} chunk, {len(cleaned_text):,} karakter"
                )

            except Exception as e:
                logger.error(f"   âŒ Ä°ÅŸleme hatasÄ± {os.path.basename(file_path)}: {e}")
                self.stats["failed_files"] += 1
                continue

        # Final istatistikler
        logger.info(f"ğŸ“Š Ä°ÅŸlem tamamlandÄ±:")
        logger.info(f"   BaÅŸarÄ±lÄ±: {self.stats['processed_files']}")
        logger.info(f"   BaÅŸarÄ±sÄ±z: {self.stats['failed_files']}")
        logger.info(f"   Toplam chunk: {self.stats['total_chunks']:,}")
        logger.info(f"   Toplam karakter: {self.stats['total_characters']:,}")

        return processed_data

    def _get_supported_files(self, path: str) -> List[str]:
        """Desteklenen dosyalarÄ± listele"""
        files = []

        if os.path.isfile(path):
            if Path(path).suffix.lower() in self.SUPPORTED_FORMATS:
                files.append(path)
        elif os.path.isdir(path):
            for root, _, filenames in os.walk(path):
                for filename in filenames:
                    if Path(filename).suffix.lower() in self.SUPPORTED_FORMATS:
                        files.append(os.path.join(root, filename))

        return sorted(files)  # Deterministic order


def save_enhanced_data(data: List[Dict[str, Any]], output_file: str) -> None:
    """GeliÅŸmiÅŸ veri kaydetme"""
    try:
        # Backup eski dosya
        if os.path.exists(output_file):
            backup_file = f"{output_file}.backup"
            os.rename(output_file, backup_file)
            logger.info(f"Eski dosya yedeklendi: {backup_file}")

        # Yeni dosyayÄ± kaydet
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… Veriler kaydedildi: {output_file}")

        # Dosya boyutu kontrolÃ¼
        file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB
        logger.info(f"ğŸ“ Dosya boyutu: {file_size:.2f} MB")

    except Exception as e:
        logger.error(f"âŒ Kaydetme hatasÄ±: {e}")
        raise


def main():
    """uploads klasÃ¶rÃ¼ndeki tÃ¼m desteklenen dosyalarÄ± iÅŸleyip uploads_base.json'a kaydeder"""
    uploads_dir = "uploads"
    output_file = "uploads_base.json"

    # Logging setup
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
    )

    try:
        logger.info("ğŸš€ uploads klasÃ¶rÃ¼ndeki dosyalar iÅŸleniyor...")

        processor = AdvancedDocumentProcessor()

        if not os.path.exists(uploads_dir):
            logger.error(f"uploads klasÃ¶rÃ¼ bulunamadÄ±: {uploads_dir}")
            return

        files_to_process = processor._get_supported_files(uploads_dir)

        if not files_to_process:
            logger.error("âŒ uploads klasÃ¶rÃ¼nde iÅŸlenebilir dosya bulunamadÄ±.")
            return

        all_data = []
        for i, file_path in enumerate(files_to_process, 1):
            logger.info(f"[{i}/{len(files_to_process)}] Ä°ÅŸleniyor: {file_path}")
            try:
                data = processor.process_documents(file_path)
                if data:
                    all_data.extend(data)
                    logger.info(f"âœ… {file_path} iÅŸlendi ve eklendi.")
                else:
                    logger.warning(f"âš ï¸ Ä°ÅŸlenemeyen dosya: {file_path}")
            except Exception as e:
                logger.error(f"âŒ Dosya iÅŸleme hatasÄ±: {file_path}, Hata: {e}")

        if all_data:
            save_enhanced_data(all_data, output_file)
            logger.info(f"âœ… TÃ¼m dosyalar {output_file} dosyasÄ±na kaydedildi.")
        else:
            logger.warning("âš ï¸ HiÃ§bir dosya baÅŸarÄ±yla iÅŸlenemedi.")

    except Exception as e:
        logger.error(f"âŒ Ana iÅŸlem hatasÄ±: {e}")
        raise


if __name__ == "__main__":
    main()