import requests
import re
import json
from typing import Dict, Any, Optional, List
from config import config
import logging
from openai import OpenAI

logger = logging.getLogger(__name__)


def temizle_yanit(yazi: str) -> str:
    """YanÄ±tÄ± temizle ve dÃ¼zenle"""
    if not yazi:
        return ""

    # <think> etiketlerini kaldÄ±r
    yazi = re.sub(r"<think>.*?</think>", "", yazi, flags=re.DOTALL | re.IGNORECASE)

    # HTML etiketlerini kaldÄ±r
    yazi = re.sub(r"<.*?>", "", yazi, flags=re.DOTALL)

    # Markdown iÅŸaretlerini temizle
    yazi = re.sub(r"\*\*|__|~~|`", "", yazi)

    # Ã‡oklu boÅŸluklarÄ± temizle
    yazi = re.sub(r"\s+", " ", yazi).strip()

    # Ã‡ok uzun yanÄ±tlarÄ± kÄ±salt
    if len(yazi) > config.MAX_ANSWER_LENGTH:
        words = yazi.split()
        yazi = " ".join(words[: config.MAX_ANSWER_LENGTH // 5]) + "..."

    return yazi


def enhanced_prompt_engineering(prompt: str, query_category: str = "general") -> str:
    """GeliÅŸmiÅŸ prompt mÃ¼hendisliÄŸi"""

    # Kategori bazlÄ± ek talimatlar
    category_instructions = {
        "procedure": "\nAdÄ±m adÄ±m aÃ§Ä±klama yapÄ±n. SÄ±ralÄ± liste halinde sunun.",
        "temporal": "\nTarih ve zaman bilgilerini kesin olarak belirtin.",
        "quantitative": "\nSayÄ±sal bilgileri tam ve doÄŸru verin.",
        "definition": "\nTanÄ±mlarÄ± aÃ§Ä±k ve anlaÅŸÄ±lÄ±r yapÄ±n.",
        "explanation": "\nSebep-sonuÃ§ iliÅŸkilerini aÃ§Ä±klayÄ±n.",
        "location": "\nYer bilgilerini spesifik belirtin.",
        "general": "\nKapsamlÄ± ve dÃ¼zenli aÃ§Ä±klama yapÄ±n.",
    }

    enhanced_prompt = prompt + category_instructions.get(query_category, "")

    # Maksimum gÃ¼Ã§lÃ¼ prompt talimatlarÄ±
    quality_instructions = """

Ã‡Ã–ZÃœLMEZ PROMPT TALÄ°MATLARI:
- Verilen belgelerdeki HER METÄ°N PARÃ‡ASÄ±nÄ± tamamen tara ve oku
- "14. Bilgisayar LaboratuvarlarÄ±nda yapÄ±lan sÄ±navlarda her tÃ¼rlÃ¼ yiyecek ve iÃ§ecek (su dÃ¢hil) bulundurulmasÄ± yasaktÄ±r" 
  gibi SAYILI MADDE KURALLARINI mutlaka bul ve belirt
- Su, yiyecek, iÃ§ecek, yasaktÄ±r, dÃ¢hil, laboratuvar gibi anahtar kelimeleri ara
- Madde numaralarÄ±nÄ± (14. madde gibi) MUTLAKA belirt  
- Kesin ve doÄŸrudan yanÄ±t ver - "galiba, sanÄ±rÄ±m" YASAK
- SADECE gerÃ§ekten hiÃ§ bilgi yoksa "belirtilmemiÅŸ" de
- Belgede kural varsa "belirtilmemiÅŸ" deme - bu YANLIÅ
- SÄ±nav kurallarÄ± HAYATI Ã–NEM - laboratuvar kurallarÄ±na DÄ°KKAT ET
- Her belge parÃ§asÄ±nda kural arama yapman GEREKÄ°YOR"""

    return enhanced_prompt + quality_instructions


def ask_local_llm(
    prompt: str,
    model: Optional[str] = None,
    query_category: str = "general",
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> str:
    """OpenAI API kullanarak LLM Ã§aÄŸrÄ±sÄ±"""

    if model is None:
        model = config.LLM_MODEL
    if temperature is None:
        temperature = config.LLM_TEMPERATURE
    if max_tokens is None:
        max_tokens = config.LLM_MAX_TOKENS

    # Prompt'u geliÅŸtir
    enhanced_prompt = enhanced_prompt_engineering(prompt, query_category)

    try:
        logger.info(f"ğŸ”„ OpenAI API Ã§aÄŸrÄ±sÄ± yapÄ±lÄ±yor - Model: {model}")

        # OpenAI client oluÅŸtur
        client = OpenAI(api_key=config.OPENAI_API_KEY, base_url=config.OPENAI_BASE_URL)

        # API Ã§aÄŸrÄ±sÄ± - maksimum kesinlik iÃ§in optimize edildi
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": enhanced_prompt}],
            temperature=0.01,  # Minimum temperature - maksimum kesinlik
            max_tokens=max_tokens,
            top_p=0.5,  # Ã‡ok dar odaklanma
            frequency_penalty=0.3,  # TekrarÄ± Ã¶nle
            presence_penalty=0.1,
        )

        raw_yanit = response.choices[0].message.content

        if not raw_yanit:
            logger.warning("âš ï¸ OpenAI'den boÅŸ yanÄ±t alÄ±ndÄ±")
            return "âš ï¸ Modelden net bir yanÄ±t alÄ±namadÄ±."

        # YanÄ±tÄ± temizle
        temiz_yanit = temizle_yanit(raw_yanit)

        # Minimum uzunluk kontrolÃ¼
        if len(temiz_yanit) < config.MIN_ANSWER_LENGTH:
            logger.warning(f"âš ï¸ Ã‡ok kÄ±sa yanÄ±t: {len(temiz_yanit)} karakter")
            return (
                "âš ï¸ Yeterince detaylÄ± yanÄ±t alÄ±namadÄ±. LÃ¼tfen daha spesifik soru sorun."
            )

        logger.info(f"âœ… OpenAI yanÄ±tÄ± alÄ±ndÄ± - {len(temiz_yanit)} karakter")
        return temiz_yanit

    except Exception as e:
        if "api_key" in str(e).lower():
            logger.error("âš ï¸ OpenAI API key hatasÄ±")
            return "âš ï¸ OpenAI API anahtarÄ± geÃ§ersiz veya eksik."
        elif "rate_limit" in str(e).lower():
            logger.error("âš ï¸ OpenAI rate limit hatasÄ±")
            return "âš ï¸ API kullanÄ±m limiti aÅŸÄ±ldÄ±. LÃ¼tfen biraz bekleyin."
        elif "insufficient_quota" in str(e).lower():
            logger.error("âš ï¸ OpenAI quota hatasÄ±")
            return "âš ï¸ API kotanÄ±z tÃ¼kendi. LÃ¼tfen hesabÄ±nÄ±zÄ± kontrol edin."
        else:
            logger.error(f"âš ï¸ OpenAI API hatasÄ±: {e}")
            return f"âš ï¸ OpenAI servisi hatasÄ±: {str(e)[:100]}"


def batch_llm_requests(
    prompts_list: List[Any], model: Optional[str] = None
) -> List[str]:
    """Ã‡oklu LLM istekleri iÃ§in batch iÅŸleme"""
    results = []

    for i, prompt_data in enumerate(prompts_list):
        if isinstance(prompt_data, dict):
            prompt = prompt_data.get("prompt", "")
            category = prompt_data.get("category", "general")
        else:
            prompt = str(prompt_data)
            category = "general"

        logger.info(f"ğŸ”„ Batch iÅŸlem {i+1}/{len(prompts_list)}")

        result = ask_local_llm(prompt, model=model, query_category=category)
        results.append(result)

    return results


def validate_llm_connection() -> Dict[str, Any]:
    """OpenAI API baÄŸlantÄ±sÄ±nÄ± doÄŸrula"""
    try:
        logger.info("ğŸ”„ OpenAI API baÄŸlantÄ±sÄ± test ediliyor...")

        # OpenAI client oluÅŸtur
        client = OpenAI(api_key=config.OPENAI_API_KEY, base_url=config.OPENAI_BASE_URL)

        # Basit bir test Ã§aÄŸrÄ±sÄ±
        response = client.chat.completions.create(
            model=config.LLM_MODEL,
            messages=[{"role": "user", "content": "Test"}],
            max_tokens=5,
        )

        # Mevcut modelleri al (opsiyonel)
        available_models = []
        try:
            models_response = client.models.list()
            available_models = [model.id for model in models_response.data]
        except:
            available_models = list(config.OPENAI_LLM_MODELS.values())

        target_model = config.LLM_MODEL
        model_available = target_model in available_models

        return {
            "connected": True,
            "service": "OpenAI API",
            "available_models": available_models[:10],  # Ä°lk 10 model
            "target_model": target_model,
            "target_model_available": model_available,
            "total_models": len(available_models),
        }

    except Exception as e:
        error_msg = str(e).lower()
        if "api_key" in error_msg or "unauthorized" in error_msg:
            return {
                "connected": False,
                "error": "OpenAI API anahtarÄ± geÃ§ersiz veya eksik",
                "suggestion": "OPENAI_API_KEY environment variable'Ä±nÄ± kontrol edin",
            }
        elif "quota" in error_msg:
            return {
                "connected": False,
                "error": "OpenAI API kotasÄ± tÃ¼kendi",
                "suggestion": "OpenAI hesabÄ±nÄ±zÄ±n kotasÄ±nÄ± kontrol edin",
            }
        else:
            return {
                "connected": False,
                "error": f"OpenAI API hatasÄ±: {str(e)[:100]}",
                "suggestion": "OpenAI konfigÃ¼rasyonunu kontrol edin",
            }


def test_llm_quality() -> Dict[str, Any]:
    """LLM yanÄ±t kalitesini test et"""
    test_prompts = [
        {
            "prompt": "SÄ±nav sistemi hakkÄ±nda kÄ±sa bilgi ver.",
            "category": "general",
            "expected_keywords": ["sÄ±nav", "deÄŸerlendirme", "not"],
        },
        {
            "prompt": "KayÄ±t iÅŸlemi nasÄ±l yapÄ±lÄ±r?",
            "category": "procedure",
            "expected_keywords": ["adÄ±m", "iÅŸlem", "kayÄ±t"],
        },
    ]

    results = []

    for test in test_prompts:
        response = ask_local_llm(test["prompt"], query_category=test["category"])

        # Keyword kontrolÃ¼
        keyword_found = any(
            keyword in response.lower() for keyword in test["expected_keywords"]
        )

        results.append(
            {
                "prompt": test["prompt"],
                "response": response[:100] + "...",  # Ä°lk 100 karakter
                "length": len(response),
                "keywords_found": keyword_found,
                "quality": "Good" if keyword_found and len(response) > 50 else "Poor",
            }
        )

    return {
        "tests": results,
        "passed": sum(1 for r in results if r["quality"] == "Good"),
        "total": len(results),
    }


if __name__ == "__main__":
    # Test iÅŸlemleri
    print("ğŸ§ª OpenAI API BaÄŸlantÄ± Testi...")
    connection_status = validate_llm_connection()
    print(f"BaÄŸlantÄ± durumu: {connection_status}")

    if connection_status.get("connected"):
        print("\nğŸ§ª LLM Kalite Testi...")
        quality_results = test_llm_quality()
        print(f"GeÃ§en testler: {quality_results['passed']}/{quality_results['total']}")

        for test in quality_results["tests"]:
            print(f"âœ… {test['prompt'][:30]}... - Kalite: {test['quality']}")
    else:
        print("âŒ OpenAI API baÄŸlantÄ±sÄ± kurulamadÄ±, kalite testi yapÄ±lamÄ±yor.")
        print(f"Hata: {connection_status.get('error', 'Bilinmeyen hata')}")
        print(
            f"Ã–neri: {connection_status.get('suggestion', 'KonfigÃ¼rasyonu kontrol edin')}"
        )